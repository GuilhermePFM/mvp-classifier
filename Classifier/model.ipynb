{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7b70e9",
   "metadata": {},
   "source": [
    "# Classifier model for personal spendings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dfcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b2709",
   "metadata": {},
   "source": [
    "## Set Env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c90dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root_dir = Path('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffde7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e023de8",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5690afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_treated_dataset\n",
    "from feature_engineering import create_new_features\n",
    "\n",
    "complete_dataset = load_treated_dataset(root_dir)\n",
    "complete_dataset = create_new_features(complete_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac568d4",
   "metadata": {},
   "source": [
    "## Split test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fadaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guilh\\Code\\ControleFinanceiro\\mvp-classifier\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# import pandas as pd\n",
    "# from typing import Tuple\n",
    "# from feature_engineering import TARGET_VARIABLE\n",
    "# X = complete_dataset[[i for i in complete_dataset if i!= TARGET_VARIABLE]]\n",
    "# y = pd.DataFrame(complete_dataset[TARGET_VARIABLE])\n",
    "# y.rename( columns = { y.columns[0]: TARGET_VARIABLE} , inplace=True)\n",
    "\n",
    "# # uses StratifiedKFold to ensure that the distribution of the target variable is preserved in both train and test sets\n",
    "# # This is particularly useful for imbalanced datasets.\n",
    "# train_indx, test_indx = next(\n",
    "#     StratifiedKFold(random_state=7, shuffle=True, n_splits=2).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c436a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "categoria",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "categoria",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e8e34aea-2994-48a0-8260-21ba5680b661",
       "rows": [
        [
         "Investimento",
         "1"
        ],
        [
         "Educação",
         "2"
        ],
        [
         "Decoração",
         "3"
        ],
        [
         "outros ganhos",
         "3"
        ],
        [
         "Outros",
         "4"
        ],
        [
         "Ocasiões especiais",
         "9"
        ],
        [
         "Gui",
         "10"
        ],
        [
         "Trabalho",
         "13"
        ],
        [
         "Entretenimento geral",
         "15"
        ],
        [
         "Autocuidado - produtos",
         "15"
        ],
        [
         "Reembolsos",
         "17"
        ],
        [
         "Presentes",
         "21"
        ],
        [
         "IT",
         "23"
        ],
        [
         "Salário",
         "26"
        ],
        [
         "Outros Ganhos",
         "28"
        ],
        [
         "Autocuidado - serviços",
         "29"
        ],
        [
         "Viagem ",
         "31"
        ],
        [
         "Bares e rolês",
         "33"
        ],
        [
         "Alimentação cotidiana",
         "50"
        ],
        [
         "Saúde",
         "62"
        ],
        [
         "Rendimentos",
         "62"
        ],
        [
         "Compras",
         "65"
        ],
        [
         "Casa",
         "81"
        ],
        [
         "Alimentação especial",
         "121"
        ],
        [
         "Transporte",
         "134"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 25
       }
      },
      "text/plain": [
       "categoria\n",
       "Investimento                1\n",
       "Educação                    2\n",
       "Decoração                   3\n",
       "outros ganhos               3\n",
       "Outros                      4\n",
       "Ocasiões especiais          9\n",
       "Gui                        10\n",
       "Trabalho                   13\n",
       "Entretenimento geral       15\n",
       "Autocuidado - produtos     15\n",
       "Reembolsos                 17\n",
       "Presentes                  21\n",
       "IT                         23\n",
       "Salário                    26\n",
       "Outros Ganhos              28\n",
       "Autocuidado - serviços     29\n",
       "Viagem                     31\n",
       "Bares e rolês              33\n",
       "Alimentação cotidiana      50\n",
       "Saúde                      62\n",
       "Rendimentos                62\n",
       "Compras                    65\n",
       "Casa                       81\n",
       "Alimentação especial      121\n",
       "Transporte                134\n",
       "Name: categoria, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c38f6d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfeature_engineering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TARGET_VARIABLE\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplete_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplete_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTARGET_VARIABLE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guilh\\Code\\ControleFinanceiro\\mvp-classifier\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\guilh\\Code\\ControleFinanceiro\\mvp-classifier\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2940\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2936\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[0;32m   2938\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m-> 2940\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2942\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m   2944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2945\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2946\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2947\u001b[0m     )\n\u001b[0;32m   2948\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\guilh\\Code\\ControleFinanceiro\\mvp-classifier\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1927\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1898\u001b[0m \n\u001b[0;32m   1899\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1926\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m-> 1927\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[0;32m   1928\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32mc:\\Users\\guilh\\Code\\ControleFinanceiro\\mvp-classifier\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2342\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   2340\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[0;32m   2341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 2342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2347\u001b[0m     )\n\u001b[0;32m   2349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[0;32m   2350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[0;32m   2353\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "from feature_engineering import TARGET_VARIABLE\n",
    "train_test_split(complete_dataset, test_size=.3, random_state=7, stratify=complete_dataset[TARGET_VARIABLE], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc50352f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((429,), (429,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indx.shape, test_indx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff1dde4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "split_train_test() missing 1 required positional argument: 'test_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_train_test\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43msplit_train_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplete_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: split_train_test() missing 1 required positional argument: 'test_size'"
     ]
    }
   ],
   "source": [
    "from training import split_train_test\n",
    "X_train, X_test, y_train, y_test = split_train_test(complete_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece92bac",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74665d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "89e46781",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_FEATURE = [\"Valor\", \"day\", \"month\"]\n",
    "CATEGORICAL_FEATURE = ['pix', 'uber', 'ifd', 'pag', 'pg', 'aplicação',\n",
    "       'salário', 'light']\n",
    "TEXT_FEATURE = \"Descrição\"\n",
    "TARGET = \"categoria\"\n",
    "FEATURES = [TEXT_FEATURE, *NUMERICAL_FEATURE, *CATEGORICAL_FEATURE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006cf9b3",
   "metadata": {},
   "source": [
    "### Standardize numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8112f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = preprocessing.StandardScaler().fit(pd.DataFrame(X_train[\"Valor\"]))\n",
    "\n",
    "# X_train_standardized = scaler.transform(pd.DataFrame(X_train[\"Valor\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_standardized =  scaler.transform(pd.DataFrame(X_test[\"Valor\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    Normalizer,\n",
    ")\n",
    "\n",
    "def preprocess_number():\n",
    "    return make_pipeline(\n",
    "        # SimpleImputer(strategy=\"median\"),\n",
    "        StandardScaler(),\n",
    "    )\n",
    "\n",
    "def preprocess_categories():\n",
    "    return make_pipeline(\n",
    "       Normalizer(),\n",
    "    )\n",
    "\n",
    "def create_preprocessor():\n",
    "    transformers = [\n",
    "        (\"num_preprocessor\", preprocess_number(), [*NUMERICAL_FEATURE]),\n",
    "        (\"cat_preprocessor\", preprocess_categories(), [*CATEGORICAL_FEATURE]),\n",
    "    ]\n",
    "\n",
    "    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
    "\n",
    "# numeric_transformer = create_preprocessor()\n",
    "# numeric_transformer.set_output(transform=\"pandas\")\n",
    "# preprocessed_num_cat_features_df = numeric_transformer.fit_transform(\n",
    "#     X_train[[*NUMERICAL_FEATURE, *CATEGORICAL_FEATURE]]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afec351",
   "metadata": {},
   "source": [
    "## Tokenize description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bdee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "# X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(pd.DataFrame(X_train['Descrição'])) \n",
    "\n",
    "\n",
    "# # X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(pd.DataFrame(X_test['Descrição'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee2276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize text in Dataset of Pytorch tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_23016\\335403553.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[TEXT_FEATURE] = X_train[TEXT_FEATURE].fillna(\"\")\n",
      "c:\\Users\\guilh\\Code\\ControleFinanceiro\\mvp-classifier\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\guilh\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|██████████| 686/686 [00:00<00:00, 11879.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "def tokenized_pytorch_tensors(\n",
    "        df: pd.DataFrame,\n",
    "        column_list: list\n",
    "    ) -> Dataset:\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    transformers_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    def tokenize(model_inputs_batch: Dataset) -> Dataset:\n",
    "        return tokenizer(\n",
    "            model_inputs_batch[TEXT_FEATURE],\n",
    "            padding=True,\n",
    "            max_length=120,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = transformers_dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        batch_size=128\n",
    "    )\n",
    "\n",
    "    tokenized_dataset.set_format(\n",
    "        \"torch\",\n",
    "        columns=column_list\n",
    "    )\n",
    "    \n",
    "    columns_to_remove = set(tokenized_dataset.column_names) - set(column_list)\n",
    "\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(list(columns_to_remove))\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "print(\"Tokenize text in Dataset of Pytorch tensors\")\n",
    "X_train[TEXT_FEATURE] = X_train[TEXT_FEATURE].fillna(\"\")\n",
    "tokenized_df = tokenized_pytorch_tensors(\n",
    "    X_train[[TEXT_FEATURE]],\n",
    "    column_list=[\"input_ids\", \"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60466d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract text feature hidden state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/686 [00:00<?, ? examples/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Map: 100%|██████████| 686/686 [00:20<00:00, 33.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with hidden state shape: (686, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "def hidden_state_from_text_inputs(df) -> pd.DataFrame:\n",
    "\n",
    "    def extract_hidden_states(batch):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        inputs = {\n",
    "            k: v.to(device)\n",
    "            for k, v in batch.items()\n",
    "            if k in tokenizer.model_input_names\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_state = model(**inputs).last_hidden_state\n",
    "            # get the CLS token, which is the first one\n",
    "            # [:, 0] gives us a row for each batch with the first column of 768 for each\n",
    "            return {\"cls_hidden_state\": last_hidden_state[:, 0].cpu().numpy()}\n",
    "\n",
    "    cls_dataset = df.map(extract_hidden_states, batched=True, batch_size=128)\n",
    "    cls_dataset.set_format(type=\"pandas\")\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        cls_dataset[\"cls_hidden_state\"].to_list(),\n",
    "        columns=[f\"feature_{n}\" for n in range(1, 769)],\n",
    "    )\n",
    "\n",
    "print(\"Extract text feature hidden state\")\n",
    "hidden_states_df = hidden_state_from_text_inputs(tokenized_df)\n",
    "print(f\"Data with hidden state shape: {hidden_states_df.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[TEXT_FEATURE] = X_train[TEXT_FEATURE].fillna(\"\")\n",
    "tokenized_df = tokenized_pytorch_tensors(\n",
    "    X_train[[TEXT_FEATURE]],\n",
    "    column_list=[\"input_ids\", \"attention_mask\"]\n",
    ")\n",
    "print(\"Extract text feature hidden state\")\n",
    "hidden_states_df = hidden_state_from_text_inputs(tokenized_df)\n",
    "print(f\"Data with hidden state shape: {hidden_states_df.shape}\") \n",
    "\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", LabelEncoder()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc07ef",
   "metadata": {},
   "source": [
    "## Classification encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a529d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "TARGET_CATEGORIES = y_train.unique().tolist()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "classification_encoder = LabelEncoder().fit(TARGET_CATEGORIES)\n",
    "# y_train_encoded = classification_encoder.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", LabelEncoder()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f6025",
   "metadata": {},
   "source": [
    "### Group all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049c8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed features and targets\n"
     ]
    }
   ],
   "source": [
    "# print(\"Saving preprocessed features and targets\")\n",
    "\n",
    "# preprocessed_data = pd.concat(\n",
    "#     [\n",
    "#         preprocessed_num_cat_features_df.reset_index(drop=True),\n",
    "#         hidden_states_df.reset_index(drop=True),\n",
    "#         pd.DataFrame(y_train_encoded).reset_index(drop=True)\n",
    "#     ],\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# preprocessed_data.rename(columns={0: TARGET}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242df811",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e1a29492",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_column_selector \u001b[38;5;28;01mas\u001b[39;00m selector\n\u001b[0;32m      6\u001b[0m X_y_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mmerge(y_train, left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[0;32m      8\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      9\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocess_number, NUMERICAL_FEATURE),\n\u001b[1;32m---> 10\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtext_transformer\u001b[49m, TEXT_FEATURE),\n\u001b[0;32m     11\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_preprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocess_categories, CATEGORICAL_FEATURE),\n\u001b[0;32m     12\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, classification_transformer, TARGET),\n\u001b[0;32m     13\u001b[0m     ]\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m clf \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m     16\u001b[0m     steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, LogisticRegression())]\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "\n",
    "X_y_train = X_train.merge(y_train, left_index=True, right_index=True)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric_transformer\", preprocess_number, NUMERICAL_FEATURE),\n",
    "        (\"text_transformer\", text_transformer, TEXT_FEATURE),\n",
    "        (\"cat_preprocessor\", preprocess_categories, CATEGORICAL_FEATURE),\n",
    "        (\"classification_transformer\", classification_transformer, TARGET),\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec49124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create the model\n",
    "gbm = GradientBoostingClassifier(\n",
    "    n_estimators=100,    # Number of boosting stages\n",
    "    learning_rate=0.1,   # Step size shrinkage\n",
    "    max_depth=3,         # Maximum depth of each tree\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "gbm.fit(preprocessed_data[[i for i in preprocessed_data if i!=TARGET]].fillna(0), preprocessed_data[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "048b168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_23016\\1463220249.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[TEXT_FEATURE] = X_test[TEXT_FEATURE].fillna(\"\")\n",
      "Map: 100%|██████████| 172/172 [00:00<00:00, 8309.38 examples/s]\n",
      "Map: 100%|██████████| 172/172 [00:04<00:00, 42.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "X_test[TEXT_FEATURE] = X_test[TEXT_FEATURE].fillna(\"\")\n",
    "tokenized_df = tokenized_pytorch_tensors(\n",
    "    X_test[[TEXT_FEATURE]],\n",
    "    column_list=[\"input_ids\", \"attention_mask\"]\n",
    ")\n",
    "preprocessed_num_cat_features_df = column_transformer.fit_transform(\n",
    "    X_test[[*NUMERICAL_FEATURE, *CATEGORICAL_FEATURE]]\n",
    ")\n",
    "hidden_states_df = hidden_state_from_text_inputs(tokenized_df)\n",
    "y_test_encoded = classification_encoder.transform(y_test)\n",
    "\n",
    "preprocessed_data = pd.concat(\n",
    "    [\n",
    "        preprocessed_num_cat_features_df.reset_index(drop=True),\n",
    "        hidden_states_df.reset_index(drop=True),\n",
    "        pd.DataFrame(y_test_encoded).reset_index(drop=True)\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "preprocessed_data.rename(columns={0: TARGET}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "57f50763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = gbm.predict(preprocessed_data[[i for i in preprocessed_data if i!=TARGET]])\n",
    "y_test = preprocessed_data[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c62bb49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6104651162790697\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82952e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# multilabel_confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b747caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "389727d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec93e6",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "X_train = preprocessed_data[[i for i in preprocessed_data if i!=TARGET]].fillna(0)\n",
    "y_train = preprocessed_data[TARGET]\n",
    "\n",
    "\n",
    "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6012562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing the new dataset\n",
    "df_test['clean_text'] = df_test['text'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
    "X_test=df_test['clean_text'] \n",
    "#converting words to numerical data using tf-idf\n",
    "X_vector=tfidf_vectorizer.transform(X_test)\n",
    "#use the best model to predict 'target' value for the new dataset \n",
    "y_predict = lr_tfidf.predict(X_vector)      \n",
    "y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "df_test['predict_prob']= y_prob\n",
    "df_test['target']= y_predict\n",
    "final=df_test[['clean_text','target']].reset_index(drop=True)\n",
    "print(final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212dd9c",
   "metadata": {},
   "source": [
    "# Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
